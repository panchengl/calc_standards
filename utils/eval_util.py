from __future__ import division, print_function
from utils.mnn_util import inference_mnn
import numpy as np
import cfg
class AverageMeter(object):
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.average = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.average = self.sum / float(self.count)


def parse_line(line):
    '''
    Given a line from the training/test txt file, return parsed info.
    line format: line_index, img_path, img_width, img_height, [box_info_1 (5 number)], ...
    return:
        line_idx: int64
        pic_path: string.
        boxes: shape [N, 4], N is the ground truth count, elements in the second
            dimension are [x_min, y_min, x_max, y_max]
        labels: shape [N]. class index.
        img_width: int.
        img_height: int
    '''
    if 'str' not in str(type(line)):
        line = line.decode()
    s = line.strip().split(' ')
    # assert len(s) > 8, 'Annotation error! Please check your annotation file. Make sure there is at least one target object in each image.'
    assert len(s) > 3, "Annotation error! Please check your annotation file. Make sure there is img_id, img_pth obj ..."
    line_idx = int(s[0])
    pic_path = s[1]
    img_width = int(s[2])
    img_height = int(s[3])
    boxes = []
    labels = []
    if len(s) > 8:
        s = s[4:]
        if len(s) % 5 != 0:
            print(line)
        assert len(s) % 5 == 0, 'Annotation error! Please check your annotation file. Maybe partially missing some coordinates?'
        box_cnt = len(s) // 5
        for i in range(box_cnt):
            label, x_min, y_min, x_max, y_max = int(s[i * 5]), float(s[i * 5 + 1]), float(s[i * 5 + 2]), float(
                s[i * 5 + 3]), float(s[i * 5 + 4])
            boxes.append([x_min, y_min, x_max, y_max])
            labels.append(label)
    else:
        boxes.append([1, 1, 1, 1])
        labels.append(cfg.class_num + 1)
    boxes = np.asarray(boxes, np.float32)
    labels = np.asarray(labels, np.int64)
    return line_idx, pic_path, boxes, labels, img_width, img_height


gt_dict = {}  # key: img_id, value: gt object list
def parse_gt_rec(gt_filename, target_img_size, letterbox_resize=True):
    '''
    parse and re-organize the gt info.
    return:
        gt_dict: dict. Each key is a img_id, the value is the gt bboxes in the corresponding img.
    '''

    global gt_dict

    if not gt_dict:
        new_width, new_height = target_img_size
        with open(gt_filename, 'r') as f:
            for line in f:
                img_id, pic_path, boxes, labels, ori_width, ori_height = parse_line(line)
                objects = []
                # if labels[0] == (cfg.class_num + 1) and (boxes[0][0] == boxes[0][1]):
                #     continue
                for i in range(len(labels)):
                    x_min, y_min, x_max, y_max = boxes[i]
                    label = labels[i]

                    if letterbox_resize:
                        resize_ratio = min(new_width / ori_width, new_height / ori_height)

                        resize_w = int(resize_ratio * ori_width)
                        resize_h = int(resize_ratio * ori_height)

                        dw = int((new_width - resize_w) / 2)
                        dh = int((new_height - resize_h) / 2)

                        objects.append([x_min * resize_ratio + dw,
                                        y_min * resize_ratio + dh,
                                        x_max * resize_ratio + dw,
                                        y_max * resize_ratio + dh,
                                        label])
                    else:
                        objects.append([x_min * new_width / ori_width,
                                        y_min * new_height / ori_height,
                                        x_max * new_width / ori_width,
                                        y_max * new_height / ori_height,
                                        label])
                gt_dict[img_id] = objects
    return gt_dict

def draw_curve(recall, precision, label_id):
    import matplotlib.pyplot as plt
    import os
    fig = plt.figure()
    ax0 = fig.add_subplot(121, title="recall_precision")
    ax0.plot(recall, precision, 'bo-', label='ap')
    fig.savefig(os.path.join('./PR_curve/', '%s_PR.jpg'%cfg.names_class[label_id]))

# The following two functions are modified from FAIR's Detectron repo to calculate mAP:
# https://github.com/facebookresearch/Detectron/blob/master/detectron/datasets/voc_eval.py
def voc_ap(rec, prec, class_id, use_07_metric=False):
    """Compute VOC AP given precision and recall. If use_07_metric is true, uses
    the VOC 07 11-point method (default:False).
    """
    # print("last result rec is: ", rec)
    # print("last result pre is: ", prec)
    ap_recall = []
    ap_precisin = []
    if use_07_metric:
        # 11 point metric
        ap = 0.
        for t in np.arange(0., 1.1, 0.1):
            if np.sum(rec >= t) == 0:
                p = 0
            else:
                p = np.max(prec[rec >= t])
            ap = ap + p / 11.
    else:
        # correct AP calculation
        # first append sentinel values at the end
        mrec = np.concatenate(([0.], rec, [1.]))
        mpre = np.concatenate(([0.], prec, [0.]))

        # compute the precision envelope
        for i in range(mpre.size - 1, 0, -1):
            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])

        # to calculate area under PR curve, look for points
        # where X axis (recall) changes value
        i = np.where(mrec[1:] != mrec[:-1])[0]

        # and sum (\Delta recall) * prec
        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])
        ap_recall = mrec[i + 1]
        ap_precisin = mpre[i + 1]
        # print("ap_recall is: ", ap_recall)
        # print("ap_precision is: ", ap_precisin)
        draw_curve(ap_recall, ap_precisin, class_id)
    return ap


def voc_eval(gt_dict, val_preds, classidx, iou_thres=0.5, use_07_metric=False):
    '''
    Top level function that does the PASCAL VOC evaluation.
    '''
    # 1.obtain gt: extract all gt objects for this class
    class_recs = {}
    npos = 0
    for img_id in gt_dict:
        R = [obj for obj in gt_dict[img_id] if obj[-1] == classidx]
        bbox = np.array([x[:4] for x in R])
        det = [False] * len(R)
        npos += len(R)
        class_recs[img_id] = {'bbox': bbox, 'det': det}
    # 2. obtain pred results
    pred = [x for x in val_preds if x[-1] == classidx]
    img_ids = [x[0] for x in pred]
    confidence = np.array([x[-2] for x in pred])
    BB = np.array([[x[1], x[2], x[3], x[4]] for x in pred])

    # 3. sort by confidence
    sorted_ind = np.argsort(-confidence)
    try:
        BB = BB[sorted_ind, :]
    except:
        print('no box, ignore')
        return 1e-6, 1e-6, 0, 0, 0
    img_ids = [img_ids[x] for x in sorted_ind]

    # 4. mark TPs and FPs
    nd = len(img_ids)
    tp = np.zeros(nd)
    fp = np.zeros(nd)

    for d in range(nd):
        # all the gt info in some image
        R = class_recs[img_ids[d]]
        bb = BB[d, :]
        ovmax = -np.Inf
        BBGT = R['bbox']

        if BBGT.size > 0:
            # calc iou
            # intersection
            ixmin = np.maximum(BBGT[:, 0], bb[0])
            iymin = np.maximum(BBGT[:, 1], bb[1])
            ixmax = np.minimum(BBGT[:, 2], bb[2])
            iymax = np.minimum(BBGT[:, 3], bb[3])
            iw = np.maximum(ixmax - ixmin + 1., 0.)
            ih = np.maximum(iymax - iymin + 1., 0.)
            inters = iw * ih

            # union
            uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) + (BBGT[:, 2] - BBGT[:, 0] + 1.) * (
                        BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)

            overlaps = inters / uni
            ovmax = np.max(overlaps)
            jmax = np.argmax(overlaps)

        if ovmax > iou_thres:
            # gt not matched yet
            if not R['det'][jmax]:
                tp[d] = 1.
                R['det'][jmax] = 1
            else:
                fp[d] = 1.
        else:
            fp[d] = 1.

    # compute precision recall
    fp = np.cumsum(fp)
    tp = np.cumsum(tp)
    rec = tp / float(npos)
    # avoid divide by zero in case the first detection matches a difficult
    # ground truth
    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)
    ap = voc_ap(rec, prec, classidx, use_07_metric)

    # return rec, prec, ap
    return npos, nd, tp[-1] / float(npos), tp[-1] / float(nd), ap


def get_preds_gpu_tf1(sess, boxes, input_data , id_list, name_list, scale_list, number,  model_dir, score_th):
    '''
    Given the y_pred of an input image, get the predicted bbox and label info.
    return:
        pred_content: 2d list.
    '''
    # assser_
    image_id = int(id_list[number])
    inference_img_name = name_list[number]
    scale_w = scale_list[number][0]
    scale_h = scale_list[number][1]
    pred_content = []
    from utils.tf_util import tf1_inference_single_img
    results = tf1_inference_single_img(sess, boxes, input_data,
                                       img_dir=inference_img_name,
                                       INPUTSIZE=cfg.img_size[0], classes=cfg.names_class, conf_th=0.3,
                                       via_flag=False)
    for bbox in results:
        if bbox[4] > score_th:
            x_min, y_min, x_max, y_max = (bbox[0]/float(scale_w)), (bbox[1]/float(scale_h)), bbox[2]/float(scale_w), bbox[3]/float(scale_h)
            score = float(bbox[4])
            label = int(bbox[5])
            pred_content.append([image_id, x_min, y_min, x_max, y_max, score, label])
    return pred_content

    # elif cfg.inference_type == "centernet":
    #     print("current version only support tf1_mnn, i will finished code after sometimes")
    #     raise NameError
    # else:
    #     print("current version only support tf1_mnn, i will finished code after sometimes")
    #     raise NameError
    # for j in range(1, cfg.class_num + 1):
    #     for bbox in results[j]:
    #         if bbox[4] > score_th:
    #             x_min, y_min, x_max, y_max = (bbox[0]/float(scale_w)), (bbox[1]/float(scale_h)), bbox[2]/float(scale_w), bbox[3]/float(scale_h)
    #             score = float(bbox[4])
    #             label = int(j -1)
    #             pred_content.append([image_id, x_min, y_min, x_max, y_max, score, label])

def get_preds_gpu_mnn(id_list, name_list, scale_list, number,  model_dir, score_th):
    '''
    Given the y_pred of an input image, get the predicted bbox and label info.
    return:
        pred_content: 2d list.
    '''
    # assser_
    image_id = int(id_list[number])
    inference_img_name = name_list[number]
    scale_w = scale_list[number][0]
    scale_h = scale_list[number][1]
    pred_content = []
    results = inference_mnn(img_dir=inference_img_name, mnn_model_dir=model_dir, INPUTSIZE=cfg.img_size[0], classes=cfg.names_class, conf_th = cfg.score_th,
                            via_flag=cfg.single_infere_img_via_flag, save_flag=cfg.single_infere_img_save_flag, save_dir=cfg.single_infere_img_save_dir)
    for bbox in results:
        if bbox[4] > score_th:
            x_min, y_min, x_max, y_max = (bbox[0]/float(scale_w)), (bbox[1]/float(scale_h)), bbox[2]/float(scale_w), bbox[3]/float(scale_h)
            score = float(bbox[4])
            label = int(bbox[5])
            pred_content.append([image_id, x_min, y_min, x_max, y_max, score, label])
    return pred_content

def get_preds_gpu_darknet(model, id_list, name_list, scale_list, number,  model_dir, score_th):
    '''
    Given the y_pred of an input image, get the predicted bbox and label info.
    return:
        pred_content: 2d list.
    '''
    # assser_
    image_id = int(id_list[number])
    inference_img_name = name_list[number]
    scale_w = scale_list[number][0]
    scale_h = scale_list[number][1]
    pred_content = []
    from utils.dark_util import darknet_inference_single_img
    results = darknet_inference_single_img(model, inference_img_name, 416, cfg.names_class, cfg.score_th, cfg.darknet_via_flag)
    for bbox in results:
        print("boxes is", bbox)
        print(bbox[4])
        if bbox[4] > score_th:
            x_min, y_min, x_max, y_max = (bbox[0]/float(scale_w)), (bbox[1]/float(scale_h)), bbox[2]/float(scale_w), bbox[3]/float(scale_h)
            score = float(bbox[4])
            label = int(bbox[5])
            pred_content.append([image_id, x_min, y_min, x_max, y_max, score, label])
    return pred_content

def get_preds_gpu_centernet(model, id_list, name_list, number, scale_list):
    '''
    Given the y_pred of an input image, get the predicted bbox and label info.
    return:
        pred_content: 2d list.
    '''
    image_id = int(id_list[number])
    inference_img_name = name_list[number]
    pred_content = []
    scale_w = scale_list[number][0]
    scale_h = scale_list[number][1]
    from utils.centernet_utils import centernet_inference_single_img
    results = centernet_inference_single_img(model, inference_img_name, cfg.darknet_via_flag)
    for j in range(1, cfg.class_num + 1):
        for bbox in results[j]:
            if bbox[4] > cfg.score_th:
                # x_min, y_min, x_max, y_max = bbox[0], bbox[1], bbox[2], bbox[3]
                x_min, y_min, x_max, y_max =(bbox[0]/float(scale_w)), (bbox[1]/float(scale_h)), bbox[2]/float(scale_w), bbox[3]/float(scale_h)
                score = bbox[4]
                label = j -1
                pred_content.append([image_id, x_min, y_min, x_max, y_max, score, label])
    return pred_content



